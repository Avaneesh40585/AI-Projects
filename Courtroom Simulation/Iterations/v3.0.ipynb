{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1H15QUUXQYDEBtuVLlvxx-o66YCzr5zBU","timestamp":1745078033105},{"file_id":"1I8VV0rXRs560cRFFFQdyp7rp77OYzrZn","timestamp":1745072668612}],"toc_visible":true,"authorship_tag":"ABX9TyM6thZmjsPapYQvrQeEbP8u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **v3.0**\n","\n","\n","*   groqcloud API\n","*   Model: llama-3.3-70b-versatile\n","*   Has all the required agents\n","*   Supports dynamic agent creation\n","*   Supports freely evolving trial structure\n","*   Predicts the verdict of test_cases.csv data\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"OWZGbzcTrALK"}},{"cell_type":"markdown","source":["### Installing groq"],"metadata":{"id":"DcZB9aWWsq1P"}},{"cell_type":"code","source":["pip install groq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLAoBcJdMjrh","executionInfo":{"status":"ok","timestamp":1745079648832,"user_tz":-330,"elapsed":5998,"user":{"displayName":"Avaneesh","userId":"01465946186716782389"}},"outputId":"90e377e5-71dd-4f13-cec5-b571328e7647"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n","Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: groq\n","Successfully installed groq-0.22.0\n"]}]},{"cell_type":"markdown","source":["## Importing Libraries"],"metadata":{"id":"va6VDquwe5Ue"}},{"cell_type":"code","source":["import pandas as pd\n","from groq import Groq\n","from typing import List, Dict, Optional\n","import re\n","import time"],"metadata":{"id":"0AI4mHXkgeIy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Choosing Model"],"metadata":{"id":"aXVQNiiugw3p"}},{"cell_type":"code","source":["GROQ_API_KEYS = [\"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\"]\n","MODEL_NAME = \"llama-3.3-70b-versatile\""],"metadata":{"id":"T044WC3dg1NB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"RjyKcYkm5JI7"}},{"cell_type":"code","source":["def parse_llm_verdict(judge_response: str):\n","    match = re.search(r\"#VERDICT:\\s*(GRANTED|DENIED)\", judge_response, re.IGNORECASE)\n","    if match:\n","        return 1 if match.group(1).upper() == \"GRANTED\" else 0\n","    if \"granted\" in judge_response.lower() or \"acquitted\" in judge_response.lower():\n","        return 1\n","    return 0"],"metadata":{"id":"YcaRnSTd4_BU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Classes Implementation"],"metadata":{"id":"I9KiEgFyhvAR"}},{"cell_type":"markdown","source":["## Courtroom Agent"],"metadata":{"id":"GboCvjMjh3Rg"}},{"cell_type":"code","source":["class CourtroomAgent:\n","    def __init__(self, name: str, role: str, system_prompt: str, client):\n","        self.name = name\n","        self.role = role\n","        self.system_prompt = system_prompt\n","        self.client = client\n","        self.history: List[Dict[str, str]] = [\n","            {\"role\": \"system\", \"content\": self.system_prompt}\n","        ]\n","    def add_message(self, role: str, content: str):\n","        self.history.append({\"role\": role, \"content\": content})\n","    def get_response(self, user_message: str) -> str:\n","        self.add_message(\"user\", user_message)\n","        response = self.client.chat.completions.create(\n","            model=MODEL_NAME, messages=self.history, stream=False)\n","        reply = response.choices[0].message.content\n","        self.add_message(\"assistant\", reply)\n","        return reply"],"metadata":{"id":"Z433WsBViC9H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Courtroom"],"metadata":{"id":"I3PaR2w1iDqz"}},{"cell_type":"code","source":["class Courtroom:\n","    def __init__(self, case_background: str, client):\n","        self.client = client\n","        self.agents: Dict[str, CourtroomAgent] = {}\n","        self.case_background: str = case_background\n","        self.case_type: str = self.determine_case_type(case_background)\n","        self.dynamic_agent_counter = 0\n","        self.add_agent(\"Judge\", CourtroomAgent(\"Judge\", \"Judge\", \"You are a fair and wise judge. Weigh all arguments neutrally and deliver clear rulings.\", client))\n","        self.add_agent(\"Defense\", CourtroomAgent(\"Defense\", \"Defense Lawyer\", \"You are the defense lawyer. Protect your client and argue logically against the prosecution's claims.\", client))\n","        self.add_agent(\"Defendant\", CourtroomAgent(\"Defendant\", \"Defendant\", \"You are the defendant. Answer questions honestly and provide your perspective on the case.\", client))\n","        self.add_agent(\"Prosecution\", CourtroomAgent(\"Prosecution\", \"Prosecution Lawyer\", \"You are the prosecution lawyer. Present compelling arguments and point out flaws in the defense.\", client))\n","        if self.case_type == \"civil\":\n","            self.add_agent(\"Plaintiff\", CourtroomAgent(\"Plaintiff\", \"Plaintiff\", \"You are the plaintiff. Represent your interests and highlight injustices suffered.\", client))\n","    def add_agent(self, key: str, agent: CourtroomAgent):\n","        self.agents[key] = agent\n","    def create_witness(self, name: Optional[str] = None, prompt: Optional[str] = None) -> str:\n","        self.dynamic_agent_counter += 1\n","        witness_name = name or f\"Witness{self.dynamic_agent_counter}\"\n","        system_prompt = prompt or f\"You are {witness_name}, a witness in this case. Answer questions truthfully and relevantly.\"\n","        self.add_agent(witness_name, CourtroomAgent(witness_name, \"Witness\", system_prompt, self.client))\n","        return witness_name\n","    def run_phase(self, phase_name: str, prompts: Dict[str, str]) -> Dict[str, str]:\n","        outputs = {}\n","        for role, prompt in prompts.items():\n","            if role in self.agents:\n","                full_prompt = prompt + \"\\nCase: \" + self.case_background[:4000]\n","                response = self.agents[role].get_response(full_prompt)\n","                outputs[role] = response\n","        return outputs\n","    @staticmethod\n","    def determine_case_type(case_background: str) -> str:\n","        civil_keywords = [\"contract\", \"arbitration\", \"plaintiff\", \"commercial\", \"company\", \"tender\", \"writ petition\", \"agreement\"]\n","        criminal_keywords = [\"murder\", \"theft\", \"assault\", \"criminal\", \"prosecution\", \"accused\", \"defendant\"]\n","        background_lower = case_background.lower()\n","        if any(word in background_lower for word in civil_keywords):\n","            return \"civil\"\n","        if any(word in background_lower for word in criminal_keywords):\n","            return \"criminal\"\n","        return \"civil\""],"metadata":{"id":"h5TvqERU21sN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompts for each phase"],"metadata":{"id":"XxgN14O8jUn8"}},{"cell_type":"code","source":["def get_opening_prompts(court):\n","    prompts = {\n","        \"Prosecution\": (\n","            \"As the prosecution lawyer, deliver a formal opening statement. Summarize the facts of the case, outline the charges, and explain the prosecution's theory of the crime. Emphasize the seriousness of the alleged offenses and preview the evidence you intend to present.\"\n","        ),\n","        \"Defense\": (\n","            \"As the defense lawyer, present a formal opening statement. Introduce your clients and their position, challenge the prosecution's narrative, and outline the defense's main arguments. Emphasize the presumption of innocence and the burden of proof.\"\n","        ),\n","        \"Defendant\": (\n","            \"As the defendant, briefly introduce yourself to the court. State your relationship to the case and your initial reaction to the charges brought against you.\"\n","        ),\n","    }\n","    if court.case_type == \"civil\":\n","        prompts[\"Plaintiff\"] = (\n","            \"As the plaintiff, deliver a formal opening statement. Explain your grievance, the harm you have suffered, and why you are seeking relief from the court. Briefly outline the evidence and arguments you will present.\"\n","        )\n","    return prompts\n","\n","def get_phase2_prompts(court, witness1=\"Witness1\", expert=\"ExpertWitness\"):\n","    prompts = {\n","        \"Prosecution\": (\n","            f\"As the prosecution lawyer, conduct a direct examination of {witness1}. Ask questions to establish the facts of the case and highlight evidence that supports the prosecution's theory.\"\n","        ),\n","        \"Defense\": (\n","            f\"As the defense lawyer, cross-examine {witness1}. Probe for inconsistencies, challenge the witness's credibility, and defend your client's position.\"\n","        ),\n","        expert: (\n","            \"As the expert witness in contract law, provide your professional analysis of the arbitration clause and its legal implications in this case.\"\n","        ),\n","        \"Defendant\": (\n","            \"As the defendant, respond to the testimonies provided by the witnesses. Clarify your actions and motivations, and address any allegations made against you.\"\n","        ),\n","    }\n","    if court.case_type == \"civil\":\n","        prompts[\"Plaintiff\"] = (\n","            \"As the plaintiff, respond to the testimonies and arguments presented so far. Clarify your position and highlight any evidence supporting your claims.\"\n","        )\n","    return prompts\n","\n","def get_closing_prompts(court):\n","    prompts = {\n","        \"Prosecution\": (\n","            \"As the prosecution lawyer, deliver a formal closing statement. Summarize the prosecution's case, review the key evidence and testimonies, and argue why the defendant should be found guilty beyond a reasonable doubt.\"\n","        ),\n","        \"Defense\": (\n","            \"As the defense lawyer, deliver a formal closing statement. Summarize your defense, highlight weaknesses in the prosecution's case, and argue for your client's acquittal.\"\n","        ),\n","        \"Defendant\": (\n","            \"As the defendant, present your final remarks to the court. Express your perspective on the trial and reiterate your innocence or mitigating circumstances.\"\n","        ),\n","    }\n","    if court.case_type == \"civil\":\n","        prompts[\"Plaintiff\"] = (\n","            \"As the plaintiff, deliver your closing statement. Summarize your case, the harm suffered, and why the court should rule in your favor.\"\n","        )\n","    return prompts\n","\n","def get_judge_ruling_prompt():\n","    return {\n","        \"Judge\": (\n","            \"As the judge, review the arguments, evidence, and testimonies presented during the trial. \"\n","            \"Deliver your verdict with clear legal reasoning, referencing the facts and applicable law. \"\n","            \"At the END of your response, write ONLY one of these tags on a new line:\\n\"\n","            \"#VERDICT: GRANTED   (if the relief/petition/appeal should be granted or the defendant is acquitted)\\n\"\n","            \"#VERDICT: DENIED    (if the relief/petition/appeal should be denied or the defendant is convicted)\\n\"\n","            \"Do NOT output anything else on the line with the tag.\"\n","        )\n","    }\n"],"metadata":{"id":"Xedq0A02gsZa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Getting the final verdict"],"metadata":{"id":"G6B9IndknEgC"}},{"cell_type":"code","source":["def run_full_trial_and_get_verdict(case_text, client):\n","    court = Courtroom(case_background=case_text, client=client)\n","    witness1 = court.create_witness(name=\"Witness1\")\n","    expert = court.create_witness(name=\"ExpertWitness\")\n","    court.run_phase(\"Opening Statements\", get_opening_prompts(court))\n","    court.run_phase(\"Witness Interrogation & Argumentation\", get_phase2_prompts(court, witness1, \"ExpertWitness\"))\n","    court.run_phase(\"Closing Statements\", get_closing_prompts(court))\n","    judge_response = court.run_phase(\"Judge's Ruling\", get_judge_ruling_prompt())[\"Judge\"]\n","    return judge_response\n","\n","def get_reset_seconds_from_error_msg(error_msg):\n","    import re\n","    match = re.search(r'try again in ([\\d\\.]+)s', error_msg)\n","    if match:\n","        return int(float(match.group(1))) + 1  # Add a buffer\n","    return 900  # Default to 15 min\n","\n","def batch_predict_no_csv(input_csv=\"test_cases.csv\", max_cases=50):\n","    df = pd.read_csv(input_csv)\n","    key_idx = 0\n","    n_keys = len(GROQ_API_KEYS)\n","    for idx, row in df.head(max_cases).iterrows():\n","        case_id = row['id'] if 'id' in row else row['ID']\n","        case_text = row.get('text') or row.get('case_text') or row[1]\n","        attempts = 0\n","        while True:\n","            api_key = GROQ_API_KEYS[key_idx]\n","            client = Groq(api_key=api_key)\n","            try:\n","                judge_response = run_full_trial_and_get_verdict(case_text, client)\n","                verdict = parse_llm_verdict(judge_response)\n","                print(f\"{case_id},{verdict}\")\n","                break  # success!\n","            except Exception as e:\n","                msg = str(e)\n","                if \"rate limit\" in msg or \"429\" in msg:\n","                    print(f\"# API key {key_idx+1} out of tokens or rate-limited.\")\n","                    key_idx = (key_idx + 1) % n_keys\n","                    attempts += 1\n","                    if attempts >= n_keys:\n","                        # All keys exhausted, so wait!\n","                        wait_time = get_reset_seconds_from_error_msg(msg)\n","                        print(f\"# All keys exhausted. Waiting {wait_time//60} min {wait_time%60}s before retrying case {case_id} ...\")\n","                        time.sleep(wait_time)\n","                        attempts = 0\n","                else:\n","                    print(f\"{case_id},0\")\n","                    print(f\"# Case {case_id} failed: {e}\")\n","                    break\n","\n","if __name__ == \"__main__\":\n","    batch_predict_no_csv(\"test_cases.csv\", max_cases=50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"Pw_DvrAag1_u","outputId":"660de3ea-0d44-43e0-b49c-c2bec73b8f90","executionInfo":{"status":"error","timestamp":1745098207721,"user_tz":-330,"elapsed":23159,"user":{"displayName":"Avaneesh","userId":"01465946186716782389"}}},"execution_count":43,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# API key 1 out of tokens or rate-limited.\n","# All keys exhausted. Waiting 15 min 0s before retrying case 1989_75 ...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-dc3c8458db18>\u001b[0m in \u001b[0;36mbatch_predict_no_csv\u001b[0;34m(input_csv, max_cases)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mjudge_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_trial_and_get_verdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mverdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_llm_verdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudge_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-dc3c8458db18>\u001b[0m in \u001b[0;36mrun_full_trial_and_get_verdict\u001b[0;34m(case_text, client)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcourt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Opening Statements\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_opening_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcourt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Witness Interrogation & Argumentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_phase2_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwitness1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ExpertWitness\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcourt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Closing Statements\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_closing_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-9fe97401a1ef>\u001b[0m in \u001b[0;36mrun_phase\u001b[0;34m(self, phase_name, prompts)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mfull_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nCase: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_background\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-95990989e2db>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, user_message)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     15\u001b[0m             model=MODEL_NAME, messages=self.history, stream=False)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01js7r0deveqbtm2dn775gy8ny` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99918, Requested 1068. Please try again in 14m11.837s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-dc3c8458db18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mbatch_predict_no_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_cases.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-dc3c8458db18>\u001b[0m in \u001b[0;36mbatch_predict_no_csv\u001b[0;34m(input_csv, max_cases)\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reset_seconds_from_error_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"# All keys exhausted. Waiting {wait_time//60} min {wait_time%60}s before retrying case {case_id} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                         \u001b[0mattempts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}